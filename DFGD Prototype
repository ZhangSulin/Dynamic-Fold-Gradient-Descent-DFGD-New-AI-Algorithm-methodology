import numpy as np

class DynamicFoldingGradientDescent:
"""
Dynamic Fold Gradient Descent (Enhanced Version):

Permanent pruning: Remove invalid features without recovery.
Gradient quantization: Low-precision processing of effective gradients.
Dynamic folding: Adaptively adjust the feature retention ratio.
"""
def __init__(self, lr=0.001, max_iter=2000, 
n_min=2, n_max=20, alpha=0.001,
prune_threshold=0.1, quant_levels=8):
self.lr = lr            
self.max_iter = max_iter  
self.n_min = n_min        
self.n_max = n_max        
self.alpha = alpha        
self.prune_threshold = prune_threshold  
self.quant_levels = quant_levels       

self.permanent_prune_mask = None  

self.theta = None       
self.n_params = None     
self.loss_history = []     

def fit(self, X, y):
d_features = X.shape[1]
self._initialize_parameters(d_features)  

for epoch in range(self.max_iter):
# 
fold_mask = self._create_fold_mask()

#linear_pred = self._forward_pass(X, fold_mask)

#
loss = self._logistic_loss(y, linear_pred)
self.loss_history.append(loss)

# 
grad_weights, grad_bias = self._backward_pass(X, y, linear_pred, fold_mask)

# permanent_prune_mask
self.permanent_prune_mask = self._apply_permanent_pruning(grad_weights)

quantized_grad = self._quantize_gradient(grad_weights)

self._update_parameters(quantized_grad, grad_bias)

self.n_params = self._adapt_n_parameters(grad_weights)

def _initialize_parameters(self, d_features):

self.theta = np.random.randn(d_features + 1)  
self.n_params = np.full(d_features, self.n_max)  
self.permanent_prune_mask = np.ones(d_features, dtype=bool) 

def _create_fold_mask(self):
active_features = self.permanent_prune_mask  
keep_probs = 1 - 1 / self.n_params[active_features]  
mask = np.zeros_like(self.permanent_prune_mask, dtype=bool)
mask[active_features] = np.random.rand(np.sum(active_features)) < keep_probs
# stay at least one mask
if np.sum(mask) == 0:
mask[np.where(active_features)[0][0]] = True  # 
return mask

def _forward_pass(self, X, mask):
active_theta = self.theta[:-1] * self.permanent_prune_mask * mask
weighted_features = X @ active_theta  #
return weighted_features + self.theta[-1]  # 

def _backward_pass(self, X, y, linear_pred, mask):
"""only for useful"""
sigmoid = 1 / (1 + np.exp(-y * linear_pred))
error = y - sigmoid
# only for not pruned
active_mask = self.permanent_prune_mask * mask
grad_weights = -np.dot(error, X * active_mask) / len(y)
grad_bias = -np.mean(error)
return grad_weights, grad_bias

def _apply_permanent_pruning(self, grad_weights):
""""""
if self.permanent_prune_mask is None:
return np.ones_like(grad_weights, dtype=bool)  # 

# 
active_grad = grad_weights[self.permanent_prune_mask]
if np.sum(active_grad) == 0:
return self.permanent_prune_mask  # 

grad_abs = np.abs(active_grad)
importance = grad_abs / grad_abs.sum()  # 
# 
prune_candidates = self.permanent_prune_mask.copy()
prune_candidates[self.permanent_prune_mask] = importance < self.prune_threshold

# 
if np.sum(prune_candidates) < 1:
prune_candidates[np.argmax(importance)] = True  # 
return prune_candidates

def _quantize_gradient(self, grad_weights):
"""for not prune"""
if self.quant_levels <= 1:  # 
return grad_weights

active_mask = self.permanent_prune_mask  # not prune
active_grad = grad_weights[active_mask]

if np.size(active_grad) == 0:
return grad_weights  # back

# [0, quant_levels-1]grad_min = np.min(active_grad)
grad_max = np.max(active_grad)
if grad_max == grad_min:
return grad_weights  # no need quantize
ale = (self.quant_levels - 1) / (grad_max - grad_min)
quantized = np.round(active_grad * scale) / scale  # liner
#
quantized_grad = grad_weights.copy()
quantized_grad[active_mask] = quantized
return quantized_grad

def _update_parameters(self, grad_weights, grad_bias):
""" update"""
active_mask = self.permanent_prune_mask  # 有效特征掩码
# update
self.theta[:-1][active_mask] -= self.lr * grad_weights[active_mask]
self.theta[-1] -= self.lr * grad_bias

def _adapt_n_parameters(self, grad_weights):
"""for not prune"""
active_mask = self.permanent_prune_mask
active_grad = grad_weights[active_mask]
if np.sum(active_grad) == 0:
return self.n_params  # 

grad_abs = np.abs(active_grad)
grad_abs[grad_abs == 0] = 1e-8  # 
importance = grad_abs / grad_abs.sum()  # importance ratio

# choose factor to be new
delta_n = self.alpha * importance
new_n = self.n_params.copy()
new_n[active_mask] = np.clip(new_n[active_mask] + delta_n, self.n_min, self.n_max)
return new_n.astype(int)  #int

def _logistic_loss(self, y, linear_pred):
""" "
linear_pred = np.clip(linear_pred, -20, 20)  #
return np.mean(np.log(1 + np.exp(-y * linear_pred)))

def predict(self, X):
"""Predict permenant mask and fold mask"""
fold_mask = self._create_fold_mask()  
active_mask = self.permanent_prune_mask * fold_mask  
linear_pred = self._forward_pass(X, active_mask)
return np.sign(linear_pred)
